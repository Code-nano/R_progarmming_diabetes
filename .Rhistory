dtrain <- xgb.DMatrix(data = as.matrix(train_data[, -ncol(train_data)]), label = train_data$Default)
dtest <- xgb.DMatrix(data = as.matrix(test_data[, -ncol(test_data)]), label = test_data$Default)
# Parameters for the xgboost model
params <- list(
booster = "gbtree",
objective = "binary:logistic",
eta = 0.3,
max_depth = 6,
eval_metric = "auc"
)
# Training the model
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 100, watchlist = list(eval = dtest), early_stopping_rounds = 10)
# Predicting on the test set
predictions <- predict(xgb_model, newdata = dtest)
# Evaluating the model
roc_result <- roc(test_data$Default, predictions)
auc(roc_result)
install.packages("xgboost")
install.packages("pR0C")
install.packages("pROC")
install.packages("pROC")
# Loading necessary libraries
library(dplyr)
# Loading necessary libraries
library(dplyr)
library(caret)
library(xgboost)
library(pROC)
set.seed(123)  # For reproducibility
# Generating sample data
sample_data <- data.frame(
Age = sample(20:70, 200, replace = TRUE),
Income = sample(25000:120000, 200, replace = TRUE),
CreditScore = sample(300:800, 200, replace = TRUE),
EmploymentLength = sample(0:30, 200, replace = TRUE),
HomeOwnership = sample(c('Own', 'Rent', 'Mortgage', 'Other'), 200, replace = TRUE),
LoanAmount = sample(5000:40000, 200, replace = TRUE),
LoanPurpose = sample(c('Debt consolidation', 'Credit card', 'Home improvement', 'Major purchase', 'Other'), 200, replace = TRUE),
DebtToIncomeRatio = runif(200, 0, 1),
Default = sample(0:1, 200, replace = TRUE)
)
# One-hot encoding for categorical variables
sample_data <- dummyVars("~ .", data = sample_data) %>%
predict(newdata = sample_data) %>%
as.data.frame()
# Splitting the data into training and testing sets
set.seed(123)
index <- createDataPartition(sample_data$Default, p = 0.8, list = FALSE)
train_data <- sample_data[index, ]
test_data <- sample_data[-index, ]
# Convert data to xgboost format
dtrain <- xgb.DMatrix(data = as.matrix(train_data[, -ncol(train_data)]), label = train_data$Default)
dtest <- xgb.DMatrix(data = as.matrix(test_data[, -ncol(test_data)]), label = test_data$Default)
# Parameters for the xgboost model
params <- list(
booster = "gbtree",
objective = "binary:logistic",
eta = 0.3,
max_depth = 6,
eval_metric = "auc"
)
# Training the model
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 100, watchlist = list(eval = dtest), early_stopping_rounds = 10)
# Predicting on the test set
predictions <- predict(xgb_model, newdata = dtest)
# Evaluating the model
roc_result <- roc(test_data$Default, predictions)
auc(roc_result)
library(ggplot2)
# Compute ROC
roc_result <- roc(test_data$Default, predictions)
# Plot ROC Curve
roc_plot <- ggplot(data = roc_result, aes(x = 1 - Specificity, y = Sensitivity)) +
geom_line(color = "blue") +
geom_abline(linetype = "dashed") +
labs(title = "ROC Curve", x = "False Positive Rate", y = "True Positive Rate") +
theme_minimal()
print(roc_plot)
# Creating a data frame for ggplot
prob_data <- data.frame(Actual = test_data$Default, Predicted_Prob = predictions)
# Plotting the distribution
prob_plot <- ggplot(prob_data, aes(x = Predicted_Prob, fill = factor(Actual))) +
geom_histogram(bins = 30, alpha = 0.6, position = 'identity') +
scale_fill_manual(values = c("red", "green"), labels = c("Default", "Non-Default")) +
labs(title = "Distribution of Predicted Probabilities", x = "Predicted Probability", y = "Count", fill = "Actual") +
theme_minimal()
print(prob_plot)
library(ggplot2)
library(tidyr)
library(dplyr)
# Generating sample data with missing values
set.seed(123)
sample_data <- data.frame(
Age = sample(20:70, 100, replace = TRUE),
Income = sample(30000:150000, 100, replace = TRUE),
CreditScore = sample(300:850, 100, replace = TRUE)
)
sample_data[sample(1:100, 20), "Income"] <- NA  # Introduce missing values
# Creating a heatmap for missing data
missing_data_heatmap <- ggplot(melt(is.na(sample_data)), aes(x = variable, y = value)) +
geom_tile(color = "white", fill = "grey") +
scale_fill_manual(values = c("grey", "yellow"), guide = FALSE) +
labs(x = "Variable", y = "Observation", title = "Missing Data Heatmap Before Preprocessing")
print(missing_data_heatmap)
library(ggplot2)
library(tidyr)
library(dplyr)
# Generating sample data with missing values
set.seed(123)
sample_data <- data.frame(
Age = sample(20:70, 100, replace = TRUE),
Income = sample(30000:150000, 100, replace = TRUE),
CreditScore = sample(300:850, 100, replace = TRUE)
)
sample_data[sample(1:100, 20), "Income"] <- NA  # Introduce missing values
# Creating a heatmap for missing data
missing_data_heatmap <- ggplot(melt(is.na(sample_data)), aes(x = variable, y = value)) +
geom_tile(color = "white", fill = "grey") +
scale_fill_manual(values = c("grey", "yellow"), guide = FALSE) +
labs(x = "Variable", y = "Observation", title = "Missing Data Heatmap Before Preprocessing")
print(missing_data_heatmap)
install.packages("reshape2")
library(reshape2)
library(ggplot2)
library(tidyr)
library(dplyr)
# Generating sample data with missing values
set.seed(123)
sample_data <- data.frame(
Age = sample(20:70, 100, replace = TRUE),
Income = sample(30000:150000, 100, replace = TRUE),
CreditScore = sample(300:850, 100, replace = TRUE)
)
sample_data[sample(1:100, 20), "Income"] <- NA  # Introduce missing values
# Creating a heatmap for missing data
missing_data_heatmap <- ggplot(melt(is.na(sample_data)), aes(x = variable, y = value)) +
geom_tile(color = "white", fill = "grey") +
scale_fill_manual(values = c("grey", "yellow"), guide = FALSE) +
labs(x = "Variable", y = "Observation", title = "Missing Data Heatmap Before Preprocessing")
print(missing_data_heatmap)
rlang::last_trace()
rlang::last_trace(drop = FALSE)
library(ggplot2)
library(tidyr)
library(dplyr)
# Generating sample data with missing values
set.seed(123)
sample_data <- data.frame(
Age = sample(20:70, 100, replace = TRUE),
Income = sample(30000:150000, 100, replace = TRUE),
CreditScore = sample(300:850, 100, replace = TRUE)
)
sample_data[sample(1:100, 20), "Income"] <- NA  # Introduce missing values
# Creating a heatmap for missing data (before handling missing values)
melted_data <- melt(is.na(sample_data))
missing_data_heatmap <- ggplot(melted_data, aes(x = variable, y = 1:nrow(melted_data))) +
geom_tile(aes(fill = value), color = "white") +
scale_fill_manual(values = c("TRUE" = "yellow", "FALSE" = "grey"), guide = FALSE) +
labs(x = "Variable", y = "Observation", title = "Missing Data Heatmap Before Preprocessing")
print(missing_data_heatmap)
library(installr)
updateR()
source("~/.active-rstudio-document")
# Chunk 1
library(tidyverse)
library(ggplot2)
library(caret)
library(dplyr)
library(Metrics)
library(e1071)
library(rpart)
library(pROC)
library(corrplot)
library(randomForest)
library(caret)
library(klaR)
# Chunk 2
data<-read_csv("diabetes.csv")
getwd()
setwd("C:\Users\Imran\Cousera\Github_repo\R_progarmming_diabetes")
setwd("C:\\Users\\Imran\\Cousera\\Github_repo\\R_progarmming_diabetes")
# Chunk 1
library(tidyverse)
library(ggplot2)
library(caret)
library(dplyr)
library(Metrics)
library(e1071)
library(rpart)
library(pROC)
library(corrplot)
library(randomForest)
library(caret)
library(klaR)
# Chunk 2
data<-read_csv("diabetes.csv")
# Chunk 3
# Summary of dataset
glimpse(data)
# Structure of dataset
str(data)
# Check if there is any NA value
colSums(is.na(data))
# Check if there is any empty string value
colSums(data=="")
# Check if there is any zero value
#colSums(data==0)
# Chunk 4
# Load the corrplot package
library(corrplot)
# Generate a random dataset for demonstration purposes
set.seed(123)
# Compute the correlation matrix
cor_matrix <- cor(data)
# Create a correlation heatmap
corrplot(cor_matrix, method = "color", type = "upper", order = "hclust", tl.cex = 0.7)
# Exploratory Data Analysis (EDA)
# New Column to determine the diabetes
data$diabetes_binary <- ifelse(data$Diabetes_012 > 0, "Diabetic", "Non-Diabetic")
data$diabetes_binary <- factor(data$diabetes_binary)
# Count the values of
category_count <- table(data$diabetes_binary)
print(category_count)
# Plot the bar chart
barplot(category_count, main = "Bar Chart of Diabetes", xlab = "Categories", ylab = "Frequency", col = "skyblue")
# Plot a pie chart
pie(category_count, main = "Pie Chart of Diabetes", col = rainbow(length(category_count)))
# Relation of Diabetes with Blood Pressure
contingency_table <- table(data$diabetes_binary, data$HighBP)
df_contingency <- as.data.frame.matrix(contingency_table)
barplot(as.matrix(df_contingency), beside = TRUE, legend.text = TRUE, col = c("red", "blue"),
main = "Relationship between Diabetes and BP Levels",
xlab = "Diabetes Status", ylab = "Count")
# Relation of High Cholestrol with Diabetes
cholesterol_counts <- table(data$HighChol)
print(cholesterol_counts)
pie(cholesterol_counts, main = "Pie Chart: Cholesterol Levels",
labels = c("Normal Cholesterol", "High Cholesterol"),
col = c("red", "lightblue"))
pie(category_count, main = "Pie Chart of Diabetes", col = rainbow(length(category_count)))
# Relation of High BP with Diabetes
contingency_table1 <- table(data$diabetes_binary, data$HighChol)
df_contingency1 <- as.data.frame.matrix(contingency_table1)
barplot(as.matrix(df_contingency1), beside = TRUE, legend.text = TRUE, col = c("red", "blue"),
main = "Relationship between Diabetes and BP Levels",
xlab = "Diabetes Status", ylab = "Count")
# Binning
bins <- c(0, 18.5, 25, 30, 35, 40, Inf)
# Create a new variable "BMI_category" based on the bins
data$BMI_category <- cut(data$BMI, bins, labels = c("Underweight", "Normal", "Overweight", "Obese I", "Obese II", "Obese III"))
# Filtering the dataset to diabetes patients
#data <- data %>% filter(data$Diabetes_012==1 | data$Diabetes_012==2 )
# Regression Model
# Exploratory Data Analysis
# BMI vs General Health
ggplot(data, aes(x = GenHlth, fill = BMI_category)) +
geom_bar(position = "dodge", stat = "count") +
labs(title = "Bar Plot of BMI Categories vs. General Health",
x = "BMI Category",
y = "Count") +
theme_minimal()
#Smoker vs diabetes
smoker_counts <- table(data$Smoker)
print(smoker_counts)
pie(smoker_counts,  main = "Smoking pie chart",col = rainbow(length(smoker_counts)))
legend("topright", c("Non-Smoker","Smoker"), cex = 0.8,
fill = rainbow(length(smoker_counts)))
contingency_table2 <- table(data$diabetes_binary, data$Smoker)
df_contingency2 <- as.data.frame.matrix(contingency_table2)
df_contingency2
barplot(as.matrix(df_contingency2), beside = TRUE, legend.text = TRUE, col = c("red", "blue"),
main = "Relationship between Diabetes and Smoking",
xlab = "Diabetes Status", ylab = "Count")
# According to this data, smoking has minor effect on diabetes
#Heavy Alcohol Consumption vs Diabetes
alcohol_counts <- table(data$HvyAlcoholConsump)
print(alcohol_counts)
pie(alcohol_counts,  main = "alcohol pie chart",col = rainbow(length(alcohol_counts)))
legend("topright", c("Non HvyAlcoholConsumption","HvyAlcoholConsumption"), cex = 0.8,
fill = rainbow(length(alcohol_counts)))
contingency_table3 <- table(data$diabetes_binary, data$HvyAlcoholConsump)
df_contingency3 <- as.data.frame.matrix(contingency_table3)
df_contingency3
barplot(as.matrix(df_contingency3), beside = TRUE, legend.text = TRUE, col = c("red", "blue"),
main = "Relationship between Diabetes and Heavy Alcohol Consumption",
xlab = "Diabetes Status", ylab = "Frequency")
#According to this data, there is insufficient data to prove that heavy alcohol effects diabetes.
#Lifestyle
#physactivity
contingency_table4 <- table(data$diabetes_binary, data$PhysActivity)
df_contingency4 <- as.data.frame.matrix(contingency_table4)
df_contingency4
barplot(as.matrix(df_contingency4), beside = TRUE, legend.text = TRUE, col = c("red", "blue"),
main = "Relationship between Diabetes and PhysActivity",
xlab = "Diabetes Status", ylab = "Count")
#Lifestyle
#fruits
contingency_table5 <- table(data$diabetes_binary, data$Fruits)
df_contingency5 <- as.data.frame.matrix(contingency_table5)
df_contingency5
barplot(as.matrix(df_contingency5), beside = TRUE, legend.text = TRUE, col = c("red", "blue"),
main = "Relationship between Diabetes and Fruits",
xlab = "Diabetes Status", ylab = "Count")
#Lifestyle
#Veggies
contingency_table6 <- table(data$diabetes_binary, data$Veggies)
df_contingency6 <- as.data.frame.matrix(contingency_table6)
df_contingency6
barplot(as.matrix(df_contingency4), beside = TRUE, legend.text = TRUE, col = c("red", "blue"),
main = "Relationship between Diabetes and Veggies",
xlab = "Diabetes Status", ylab = "Count")
#Lifestyle practises such as physical activity and consumption of fruits and veggies reduces the risk of diabetes
#Gender and diabetes
ggplot(data, aes(x = Sex, fill = diabetes_binary)) +
geom_bar(position = "dodge", color = "black") +
labs(title = 'Diabetes Disease Frequency for Sex',
x = 'Sex (0 = Female, 1 = Male)',
y = 'Frequency') +
scale_fill_manual(values = c('#AA1111', '#1CA53B'),
name = "Diabetes Status",
labels = c("Have Diabetic", "Healthy")) +
theme_minimal()
#We can see that female are not just more diabetic than men but they are also healthier
#Relationship between bmi and diabetes
ggplot(data, aes(x = BMI, fill = diabetes_binary)) +
geom_bar(position = "dodge", color = "black") +
labs(title = 'Diabetes Disease Frequency for BMI',
x = 'BMI',
y = 'Frequency') +
scale_fill_manual(values = c('#AA1111', '#1CA53B'),
name = "Diabetes Status",
labels = c("Have Diabetic", "Healthy")) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 0, hjust = 1))
#People range between 24-33 are more prone to have diabetes
#relationship between income and diabetes
ggplot(data, aes(x = Income, fill = diabetes_binary)) +
geom_bar(position = "dodge", color = "black") +
labs(title = 'Diabetes Disease Frequency for Income',
x = 'Income',
y = 'Frequency') +
scale_fill_manual(values = c('#AA1111', '#1CA53B'),
name = "Diabetes Status",
labels = c("Have Diabetic", "Healthy")) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 0, hjust = 1))
#We can say that people with higher income are more diabetic
# Chunk 5
# Filtering the dataset to non-diabetes and pre-diabetes patients
data <- data %>% filter(data$Diabetes_012==0 | data$Diabetes_012==1 )
# Mutate new column that combine all health complications
data <- mutate(data, "Health_Complication" = HighChol + HighBP + Stroke + HeartDiseaseorAttack)
# Define the model formula
model_formula <- Health_Complication ~ .
set.seed(123)  # Set a random seed for reproducibility
train_index <- createDataPartition(data$Health_Complication, p = 0.6, list = FALSE)
train_data <- data[train_index, ]
temp_data <- data[-train_index, ]
# Create validation and test sets
validation_index <- createDataPartition(temp_data$Health_Complication, p = 0.5, list = FALSE)
validation_data <- temp_data[validation_index, ]
test_data <- temp_data[-validation_index, ]
# Chunk 6
#Linear Regression of Health Complication with Diabetes
# Train the linear regression model
linear_model <- lm(model_formula, data = train_data)
# Summarize the model
summary(linear_model)
# Make predictions on the test set
predictions <- predict(linear_model, newdata = test_data)
# Evaluate the model performance
mse <- mean((predictions - test_data$Health_Complication)^2)
rmse <- sqrt(mse)
print(paste("Mean Squared Error:", mse))
print(paste("Root Mean Squared Error:", rmse))
# Chunk 7
#Decision Tree Regression
tree_model <- rpart(Health_Complication ~ . , data = train_data, method = "anova")
tree_prediction <- predict(tree_model, newdata = test_data)
mse <- mean((tree_prediction - test_data$Health_Complication)^2)
rmse <- sqrt(mse)
print(paste("Mean Squared Error:", mse))
print(paste("Root Mean Squared Error:", rmse))
# Chunk 8
# SVM Model
svm_model <- svm(Health_Complication~., data = train_data)
svm_prediction <- predict(svm_model, newdata = test_data)
mse <- mean((predictions - test_data$Health_Complication)^2)
rmse <- sqrt(mse)
print(paste("Mean Squared Error:", mse))
print(paste("Root Mean Squared Error:", rmse))
# Chunk 9
diabetes <-read.csv("diabetes.csv", nrows=100)  #change this later
View(diabetes)
diabetes$Diabetes_012 <- as.factor(diabetes$Diabetes_012)
glimpse(diabetes)
KNN_diabetes<-function(s, df, col) {
trainIndex<-createDataPartition(col, p=s, list=F)
data_train<-df[trainIndex,]
data_test<-df[-trainIndex,]
model <- train(Diabetes_012~., data=data_train, method = "knn")
# make predictions
x_test <- data_test[,2:22]
y_test <- data_test[,1]
predictions <- predict(model, x_test)
cm_diabetes<-confusionMatrix(predictions, y_test)
return(list(model,cm_diabetes))
}
# 80%/20% train/test
split<-0.80
result1d<-KNN_diabetes(split, diabetes, diabetes$Diabetes_012)
result1d
# 70%/30% train/test
split<-0.70
result2d<-KNN_diabetes(split, diabetes, diabetes$Diabetes_012)
result2d
#k-fold cross validation
# k-fold
diabetes <- data.frame(diabetes)
train_control <- trainControl(method="cv", number=10)
modelKNN1 <- train(Diabetes_012~., data=diabetes, trControl=train_control, method="knn")
# repeated K Fold
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
modelKNN2 <- train(Diabetes_012~., data=diabetes, trControl=train_control, method="knn")
# Leave one out CV
train_control <- trainControl(method="LOOCV")
modelKNN3 <- train(Diabetes_012~., data=diabetes, trControl=train_control, method="knn")
# bootstrap
train_control <- trainControl(method="boot", number=100)
modelKNN4 <- train(Diabetes_012~., data=diabetes, trControl=train_control, method="knn")
## make predictions
x_test <- diabetes[,2:22]
y_test <- diabetes[,1]
predictKNN1 <- predict(modelKNN1, x_test)
cmKNN1<-confusionMatrix(predictKNN1, y_test)
cmKNN1
predictKNN2 <- predict(modelKNN2, x_test)
cmKNN2<-confusionMatrix(predictKNN2, y_test)
cmKNN2
predictKNN3 <- predict(modelKNN3, x_test)
cmKNN3<-confusionMatrix(predictKNN3, y_test)
cmKNN3
predictKNN4 <- predict(modelKNN4, x_test)
cmKNN4<-confusionMatrix(predictKNN4, y_test)
cmKNN4
#ROC
split<-0.80
glimpse(diabetes)
trainIndex<-createDataPartition(diabetes$Diabetes_012, p=split, list=F)
data_train<-diabetes[trainIndex,]
data_test<-diabetes[-trainIndex,]
ctrl <- trainControl(method = "repeatedcv", repeats = 3, classProbs = TRUE, summaryFunction = twoClassSummary)
ctrl
# Chunk 10
#RANDOM FOREST
RF<-function(s, df, col) {
trainIndex<-createDataPartition(col, p=s, list=F)
data_train<-diabetes[trainIndex,]
data_test<-diabetes[-trainIndex,]
model <- randomForest(Diabetes_012~., data=data_train)
# make predictions
x_test <- data_test[,2:22]
y_test <- data_test[,1]
predictions <- predict(model, x_test)
cm<-confusionMatrix(predictions, y_test)
return(cm)
}
# 80%/20% train/test
split<-0.80
result3d<-RF(split, diabetes, diabetes$Diabetes_012)
result3d
# 70%/30% train/test
split<-0.70
result4d<-RF(split, diabetes, diabetes$Diabetes_012)
result4d
#k-fold cross validation
# k-fold
diabetes <- data.frame(diabetes)
train_control <- trainControl(method="cv", number=10)
modelRF1 <- train(Diabetes_012~., data=diabetes, trControl=train_control, method="rf")
# repeated K Fold
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
modelRF2 <- train(Diabetes_012~., data=diabetes, trControl=train_control, method="rf")
# Leave one out CV
train_control <- trainControl(method="LOOCV")
modelRF3 <- train(Diabetes_012~., data=diabetes, trControl=train_control, method="rf")
# bootstrap
train_control <- trainControl(method="boot", number=100)
modelRF4 <- train(Diabetes_012~., data=diabetes, trControl=train_control, method="rf")
## make predictions
x_test <- diabetes[,2:22]
y_test <- diabetes[,1]
predictRF1 <- predict(modelRF1, x_test)
cmRF1<-confusionMatrix(predictRF1, y_test)
cmRF1
predictRF2 <- predict(modelRF2, x_test)
cmRF2<-confusionMatrix(predictRF2, y_test)
cmRF2
predictRF3 <- predict(modelRF3, x_test)
cmRF3<-confusionMatrix(predictRF3, y_test)
cmRF3
predictRF4 <- predict(modelRF4, x_test)
cmRF4<-confusionMatrix(predictRF4, y_test)
cmRF4
#ROC
split<-0.80
glimpse(diabetes)
trainIndex<-createDataPartition(diabetes$Diabetes_012, p=split, list=F)
data_train<-diabetes[trainIndex,]
data_test<-diabetes[-trainIndex,]
ctrl <- trainControl(method = "repeatedcv", repeats = 3, classProbs = TRUE, summaryFunction = twoClassSummary)
ctrl
