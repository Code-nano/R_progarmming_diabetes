# Histogram for BMI by Diabetes Category
ggplot(data, aes(x = BMI, fill = Diabetes_012)) +
geom_histogram(bins = 30) +
facet_wrap(~Diabetes_012) +
ggtitle("BMI Distribution by Diabetes Category") +
xlab("BMI") +
ylab("Count")
# Correlation Matrix for Numeric Features
numeric_data <- data %>% select_if(is.numeric)
correlationMatrix <- cor(numeric_data)
corrplot(correlationMatrix, method = "circle",
title = "Correlation Matrix for Numeric Features")
library(tidyverse)
library(ggplot2)
library(caret)
library(dplyr)
library(Metrics)
library(e1071)
library(rpart)
library(pROC)
library(corrplot)
library(randomForest)
# Load and Prepare Data
data <- read.csv("diabetes.csv")
data$Diabetes_012 <- as.factor(data$Diabetes_012)
# Handling Missing Values
data <- na.omit(data)
# Exploratory Data Analysis
print("Exploratory Data Analysis")
# Histogram for BMI by Diabetes Category
ggplot(data, aes(x = BMI, fill = Diabetes_012)) +
geom_histogram(bins = 30) +
facet_wrap(~Diabetes_012) +
ggtitle("BMI Distribution by Diabetes Category") +
xlab("BMI") +
ylab("Count")
# Correlation Matrix for Numeric Features
numeric_data <- data %>% select_if(is.numeric)
correlationMatrix <- cor(numeric_data)
corrplot(correlationMatrix, method = "circle",
title = "Correlation Matrix for Numeric Features")
# Splitting the Data
set.seed(123)
splitIndex <- createDataPartition(data$Diabetes_012, p = 0.7, list = FALSE)
trainData <- data[splitIndex,]
testData <- data[-splitIndex,]
testData$Diabetes_012 <- as.factor(testData$Diabetes_012)
# Load and Prepare Data
data <- read.csv("diabetes.csv")
data$Diabetes_012 <- as.factor(data$Diabetes_012)
# Handling Missing Values
data <- na.omit(data)
# Exploratory Data Analysis
# Histogram for BMI by Diabetes Category
ggplot(data, aes(x = BMI, fill = Diabetes_012)) +
geom_histogram(bins = 30) +
facet_wrap(~Diabetes_012) +
ggtitle("BMI Distribution by Diabetes Category") +
xlab("BMI") +
ylab("Count")
# Correlation Matrix for Numeric Features
numeric_data <- data %>% select_if(is.numeric)
correlationMatrix <- cor(numeric_data)
corrplot(correlationMatrix, method = "circle",
title = "Correlation Matrix for Numeric Features")
# Splitting the Data
set.seed(123)
splitIndex <- createDataPartition(data$Diabetes_012, p = 0.7, list = FALSE)
trainData <- data[splitIndex,]
testData <- data[-splitIndex,]
testData$Diabetes_012 <- as.factor(testData$Diabetes_012)
# Model Building
set.seed(123)  # For reproducibility
model_rf <- train(Diabetes_012 ~ ., data = trainData, method = "rf",
trControl = trainControl(method = "cv", number = 10, verboseIter = TRUE))
# Load and Prepare Data
data <- read.csv("diabetes.csv")
data$Diabetes_012 <- as.factor(data$Diabetes_012)
# Handling Missing Values
data <- na.omit(data)
# Exploratory Data Analysis
# Histogram for BMI by Diabetes Category
ggplot(data, aes(x = BMI, fill = Diabetes_012)) +
geom_histogram(bins = 30) +
facet_wrap(~Diabetes_012) +
ggtitle("BMI Distribution by Diabetes Category") +
xlab("BMI") +
ylab("Count")
# Correlation Matrix for Numeric Features
numeric_data <- data %>% select_if(is.numeric)
correlationMatrix <- cor(numeric_data)
corrplot(correlationMatrix, method = "circle", title = "Correlation Matrix for Numeric Features", type = "upper", order = "hclust", tl.cex = 0.7)
# Splitting the Data
set.seed(123)
splitIndex <- createDataPartition(data$Diabetes_012, p = 0.7, list = FALSE)
trainData <- data[splitIndex,]
testData <- data[-splitIndex,]
testData$Diabetes_012 <- as.factor(testData$Diabetes_012)
# Load and Prepare Data
data <- read.csv("diabetes.csv")
data$Diabetes_012 <- as.factor(data$Diabetes_012)
# Handling Missing Values
data <- na.omit(data)
# Exploratory Data Analysis
# Histogram for BMI by Diabetes Category
ggplot(data, aes(x = BMI, fill = Diabetes_012)) +
geom_histogram(bins = 30) +
facet_wrap(~Diabetes_012) +
ggtitle("BMI Distribution by Diabetes Category") +
xlab("BMI") +
ylab("Count")
# Correlation Matrix for Numeric Features
numeric_data <- data %>% select_if(is.numeric)
correlationMatrix <- cor(numeric_data)
corrplot(correlationMatrix, title = "Correlation Matrix for Numeric Features", type = "upper", order = "hclust", tl.cex = 0.7)
# Splitting the Data
set.seed(123)
splitIndex <- createDataPartition(data$Diabetes_012, p = 0.7, list = FALSE)
trainData <- data[splitIndex,]
testData <- data[-splitIndex,]
testData$Diabetes_012 <- as.factor(testData$Diabetes_012)
rm(list=ls())
library(tidyverse)
library(ggplot2)
library(caret)
library(dplyr)
library(Metrics)
library(e1071)
library(rpart)
library(pROC)
library(corrplot)
library(randomForest)
data<-read_csv("diabetes.csv")
# Load and Prepare Data
data <- read.csv("diabetes.csv")
data$Diabetes_012 <- as.factor(data$Diabetes_012)
# Handling Missing Values
data <- na.omit(data)
# Exploratory Data Analysis
# Histogram for BMI by Diabetes Category
ggplot(data, aes(x = BMI, fill = Diabetes_012)) +
geom_histogram(bins = 30) +
facet_wrap(~Diabetes_012) +
ggtitle("BMI Distribution by Diabetes Category") +
xlab("BMI") +
ylab("Count")
# Correlation Matrix for Numeric Features
numeric_data <- data %>% select_if(is.numeric)
correlationMatrix <- cor(numeric_data)
corrplot(correlationMatrix, title = "Correlation Matrix for Numeric Features", type = "upper", order = "hclust", tl.cex = 0.7)
# Splitting the Data
set.seed(123)
splitIndex <- createDataPartition(data$Diabetes_012, p = 0.7, list = FALSE)
trainData <- data[splitIndex,]
testData <- data[-splitIndex,]
testData$Diabetes_012 <- as.factor(testData$Diabetes_012)
# Model Building
set.seed(123)  # For reproducibility
model_rf <- train(Diabetes_012 ~ ., data = trainData, method = "rf", trControl = trainControl(method = "cv", number = 10, verboseIter = TRUE))
# Model Tuning (Hyperparameter Optimization)
tuned_model <- tuneRF(trainData[,-1], trainData$Diabetes_012, stepFactor = 1.5, improve = 0.01, ntreeTry = 500, trace = TRUE)
# Model Evaluation
predictions <- predict(model_rf, testData)
confMat <- confusionMatrix(predictions, testData$Diabetes_012)
roc_curve <- roc(response = testData$Diabetes_012, predictor = as.numeric(predictions), levels = levels(testData$Diabetes_012))
# Model Evaluation
predictions <- predict(model_rf, testData)
confMat <- confusionMatrix(predictions, testData$Diabetes_012)
# Function to calculate ROC and AUC for each classification
calculate_roc_auc <- function(class_label, predictions, actual_labels) {
# Converting the actual labels to binary (1 for class of interest, 0 for others)
binary_response <- as.numeric(actual_labels == class_label)
# Getting predicted probabilities for the class of interest
predicted_probabilities <- predictions[, as.character(class_label)]
# Calculating ROC curve
roc_curve <- roc(response = binary_response, predictor = predicted_probabilities)
# Calculating AUC
auc_value <- auc(roc_curve)
return(list("roc_curve" = roc_curve, "auc" = auc_value))
}
# Calculating ROC and AUC for each class
roc_results <- lapply(levels(testData$Diabetes_012), calculate_roc_auc, predictions, testData$Diabetes_012)
# Model Evaluation
predictions <- predict(model_rf, testData)
confMat <- confusionMatrix(predictions, testData$Diabetes_012)
# Function to calculate ROC and AUC for each class
calculate_roc_auc <- function(class_label, predictions, actual_labels) {
# Check if predictions is a dataframe and convert it to matrix
if (is.data.frame(predictions)) {
predictions <- data.matrix(predictions)
}
# Ensure class labels in predictions match those in actual_labels
if (!class_label %in% colnames(predictions)) {
stop(paste("Class label", class_label, "not found in predictions columns."))
}
# Converting the actual labels to binary format
binary_response <- as.numeric(actual_labels == class_label)
# Getting predicted probabilities for the class of interest
predicted_probabilities <- predictions[, class_label]
# Calculating ROC curve and AUC
roc_curve <- roc(response = binary_response, predictor = predicted_probabilities)
auc_value <- auc(roc_curve)
return(list("roc_curve" = roc_curve, "auc" = auc_value))
}
# Calculating ROC and AUC for each class
roc_results <- lapply(levels(testData$Diabetes_012), calculate_roc_auc, predictions, testData$Diabetes_012)
print(predictions)
# Model Evaluation
predictions <- predict(model_rf, testData, type = "prob")
confMat <- confusionMatrix(predictions, testData$Diabetes_012)
# Model Evaluation
# Convert predictions to a factor with the same levels as in the test data
predictions <- factor(predictions, levels = levels(testData$Diabetes_012))
# Confusion Matrix
confMat <- confusionMatrix(predictions, testData$Diabetes_012)
length(predictions)
length(testData$Diabetes_012)
# Model Evaluation
# Generate class predictions
predictions <- predict(model_rf, testData)
# Generate predicted probabilities
prob_predictions <- predict(model_rf, testData, type = "prob")
# Ensure predictions are factors with the same levels as the actual data
predictions <- factor(predictions, levels = levels(testData$Diabetes_012))
# Confusion Matrix
confMat <- confusionMatrix(predictions, testData$Diabetes_012)
print(confMat)
# Overall Accuracy
accuracy <- sum(predictions == testData$Diabetes_012) / length(predictions)
print(paste("Accuracy:", accuracy))
# Function to calculate ROC and AUC for each class
calculate_roc_auc <- function(class, predictions, actual_labels) {
binary_response <- as.numeric(actual_labels == class)
roc_curve <- roc(binary_response, predictions[, as.character(class)])
auc_value <- auc(roc_curve)
list("roc_curve" = roc_curve, "auc" = auc_value)
}
# Calculating ROC and AUC for each class
roc_results <- lapply(levels(testData$Diabetes_012), calculate_roc_auc, prob_predictions, testData$Diabetes_012)
# Printing AUC values for each class
sapply(roc_results, function(x) x$auc)
# Precision, Recall, and F1-Score
precision <- posPredValue(predictions, testData$Diabetes_012)
# Model Evaluation
# Generate class predictions
predictions <- predict(model_rf, testData)
# Generate predicted probabilities
prob_predictions <- predict(model_rf, testData, type = "prob")
# Ensure predictions are factors with the same levels as the actual data
predictions <- factor(predictions, levels = levels(testData$Diabetes_012))
# Confusion Matrix
confMat <- confusionMatrix(predictions, testData$Diabetes_012)
print(confMat)
# Overall Accuracy
accuracy <- sum(predictions == testData$Diabetes_012) / length(predictions)
print(paste("Accuracy:", accuracy))
# Function to calculate ROC and AUC for each class
calculate_roc_auc <- function(class, predictions, actual_labels) {
binary_response <- as.numeric(actual_labels == class)
roc_curve <- roc(binary_response, predictions[, as.character(class)])
auc_value <- auc(roc_curve)
list("roc_curve" = roc_curve, "auc" = auc_value)
}
# Calculating ROC and AUC for each class
roc_results <- lapply(levels(testData$Diabetes_012), calculate_roc_auc, prob_predictions, testData$Diabetes_012)
# Printing AUC values for each class
sapply(roc_results, function(x) x$auc)
# Precision, Recall, and F1-Score
# Function to calculate precision, recall, and F1-score for a given class
calculate_metrics <- function(class, predictions, actual_labels) {
# Convert to binary format (one-vs-rest approach)
binary_predictions <- as.numeric(predictions == class)
binary_actual_labels <- as.numeric(actual_labels == class)
# Calculate precision, recall, and F1-score
precision <- posPredValue(binary_predictions, binary_actual_labels, positive = 1)
recall <- sensitivity(binary_predictions, binary_actual_labels, positive = 1)
f1_score <- ifelse((precision + recall) > 0, (2 * precision * recall) / (precision + recall), 0)
return(c(Precision = precision, Recall = recall, F1_Score = f1_score))
}
# Calculating metrics for each class
class_metrics <- sapply(levels(predictions), function(class) calculate_metrics(class, predictions, testData$Diabetes_012))
# Model Evaluation
# Generate class predictions
predictions <- predict(model_rf, testData)
# Generate predicted probabilities
prob_predictions <- predict(model_rf, testData, type = "prob")
# Ensure predictions are factors with the same levels as the actual data
predictions <- factor(predictions, levels = levels(testData$Diabetes_012))
# Confusion Matrix
confMat <- confusionMatrix(predictions, testData$Diabetes_012)
print(confMat)
# Overall Accuracy
accuracy <- sum(predictions == testData$Diabetes_012) / length(predictions)
print(paste("Accuracy:", accuracy))
# Function to calculate ROC and AUC for each class
calculate_roc_auc <- function(class, predictions, actual_labels) {
binary_response <- as.numeric(actual_labels == class)
roc_curve <- roc(binary_response, predictions[, as.character(class)])
auc_value <- auc(roc_curve)
list("roc_curve" = roc_curve, "auc" = auc_value)
}
# Calculating ROC and AUC for each class
roc_results <- lapply(levels(testData$Diabetes_012), calculate_roc_auc, prob_predictions, testData$Diabetes_012)
# Printing AUC values for each class
sapply(roc_results, function(x) x$auc)
# Precision, Recall, and F1-Score
# Function to calculate precision, recall, and F1-score for a given class
calculate_metrics <- function(class, predictions, actual_labels) {
# Convert to binary format (one-vs-rest approach) and then to factors
binary_predictions <- factor(as.numeric(predictions == class), levels = c(0, 1))
binary_actual_labels <- factor(as.numeric(actual_labels == class), levels = c(0, 1))
# Calculate precision, recall, and F1-score
precision <- posPredValue(binary_predictions, binary_actual_labels, positive = "1")
recall <- sensitivity(binary_predictions, binary_actual_labels, positive = "1")
f1_score <- ifelse((precision + recall) > 0, (2 * precision * recall) / (precision + recall), 0)
return(c(Precision = precision, Recall = recall, F1_Score = f1_score))
}
# Calculating metrics for each class
class_metrics <- sapply(levels(predictions), function(class) calculate_metrics(class, predictions, testData$Diabetes_012))
# Print the metrics for each class
print(class_metrics)
calculate_metrics <- function(class, predictions, actual_labels) {
# Convert to binary format (one-vs-rest approach) and then to factors
binary_predictions <- factor(as.numeric(predictions == class), levels = c(0, 1))
binary_actual_labels <- factor(as.numeric(actual_labels == class), levels = c(0, 1))
# Calculate precision, recall, and F1-score
precision <- posPredValue(binary_predictions, binary_actual_labels, positive = "1")
recall <- sensitivity(binary_predictions, binary_actual_labels, positive = "1")
f1_score <- ifelse((precision + recall) > 0, (2 * precision * recall) / (precision + recall), 0)
return(c(Precision = precision, Recall = recall, F1_Score = f1_score))
}
# Calculating metrics for each class
class_metrics <- sapply(levels(predictions), function(class) calculate_metrics(class, predictions, testData$Diabetes_012))
# Print the metrics for each class
print(class_metrics)
# Feature Importance
importance_matrix <- importance(model_rf)
# Model Evaluation
# Generate class predictions
predictions <- predict(model_rf, testData)
# Generate predicted probabilities
prob_predictions <- predict(model_rf, testData, type = "prob")
# Ensure predictions are factors with the same levels as the actual data
predictions <- factor(predictions, levels = levels(testData$Diabetes_012))
# Confusion Matrix
confMat <- confusionMatrix(predictions, testData$Diabetes_012)
print(confMat)
# Overall Accuracy
accuracy <- sum(predictions == testData$Diabetes_012) / length(predictions)
print(paste("Accuracy:", accuracy))
# Function to calculate ROC and AUC for each class
calculate_roc_auc <- function(class, predictions, actual_labels) {
binary_response <- as.numeric(actual_labels == class)
roc_curve <- roc(binary_response, predictions[, as.character(class)])
auc_value <- auc(roc_curve)
list("roc_curve" = roc_curve, "auc" = auc_value)
}
# Calculating ROC and AUC for each class
roc_results <- lapply(levels(testData$Diabetes_012), calculate_roc_auc, prob_predictions, testData$Diabetes_012)
# Printing AUC values for each class
sapply(roc_results, function(x) x$auc)
# Precision, Recall, and F1-Score
# Function to calculate precision, recall, and F1-score for a given class
calculate_metrics <- function(class, predictions, actual_labels) {
# Convert to binary format (one-vs-rest approach) and then to factors
binary_predictions <- factor(as.numeric(predictions == class), levels = c(0, 1))
binary_actual_labels <- factor(as.numeric(actual_labels == class), levels = c(0, 1))
# Calculate precision, recall, and F1-score
precision <- posPredValue(binary_predictions, binary_actual_labels, positive = "1")
recall <- sensitivity(binary_predictions, binary_actual_labels, positive = "1")
f1_score <- ifelse((precision + recall) > 0, (2 * precision * recall) / (precision + recall), 0)
return(c(Precision = precision, Recall = recall, F1_Score = f1_score))
}
# Calculating metrics for each class
class_metrics <- sapply(levels(predictions), function(class) calculate_metrics(class, predictions, testData$Diabetes_012))
# Print the metrics for each class
print(class_metrics)
calculate_metrics <- function(class, predictions, actual_labels) {
# Convert to binary format (one-vs-rest approach) and then to factors
binary_predictions <- factor(as.numeric(predictions == class), levels = c(0, 1))
binary_actual_labels <- factor(as.numeric(actual_labels == class), levels = c(0, 1))
# Calculate precision, recall, and F1-score
precision <- posPredValue(binary_predictions, binary_actual_labels, positive = "1")
recall <- sensitivity(binary_predictions, binary_actual_labels, positive = "1")
f1_score <- ifelse((precision + recall) > 0, (2 * precision * recall) / (precision + recall), 0)
return(c(Precision = precision, Recall = recall, F1_Score = f1_score))
}
# Calculating metrics for each class
class_metrics <- sapply(levels(predictions), function(class) calculate_metrics(class, predictions, testData$Diabetes_012))
# Print the metrics for each class
print(class_metrics)
# Feature Importance
# Extracting the Random Forest model from the 'train' object
rf_model <- model_rf$finalModel
# Getting the importance matrix
importance_matrix <- importance(rf_model)
print(importance_matrix)
# variable importance plot
varImpPlot(rf_model)
# Model Evaluation
# Generate class predictions
predictions <- predict(model_rf, testData)
# Generate predicted probabilities
prob_predictions <- predict(model_rf, testData, type = "prob")
# Ensure predictions are factors with the same levels as the actual data
predictions <- factor(predictions, levels = levels(testData$Diabetes_012))
# Confusion Matrix
confMat <- confusionMatrix(predictions, testData$Diabetes_012)
print(confMat)
# Overall Accuracy
accuracy <- sum(predictions == testData$Diabetes_012) / length(predictions)
print(paste("Accuracy:", accuracy))
# Function to calculate ROC and AUC for each class
calculate_roc_auc <- function(class, predictions, actual_labels) {
binary_response <- as.numeric(actual_labels == class)
roc_curve <- roc(binary_response, predictions[, as.character(class)])
auc_value <- auc(roc_curve)
list("roc_curve" = roc_curve, "auc" = auc_value)
}
# Calculating ROC and AUC for each class
roc_results <- lapply(levels(testData$Diabetes_012), calculate_roc_auc, prob_predictions, testData$Diabetes_012)
# Printing AUC values for each class
sapply(roc_results, function(x) x$auc)
# Precision, Recall, and F1-Score
# Function to calculate precision, recall, and F1-score for a given class
calculate_metrics <- function(class, predictions, actual_labels) {
# Convert to binary format (one-vs-rest approach) and then to factors
binary_predictions <- factor(as.numeric(predictions == class), levels = c(0, 1))
binary_actual_labels <- factor(as.numeric(actual_labels == class), levels = c(0, 1))
# Calculate precision, recall, and F1-score
precision <- posPredValue(binary_predictions, binary_actual_labels, positive = "1")
recall <- sensitivity(binary_predictions, binary_actual_labels, positive = "1")
f1_score <- ifelse((precision + recall) > 0, (2 * precision * recall) / (precision + recall), 0)
return(c(Precision = precision, Recall = recall, F1_Score = f1_score))
}
# Calculating metrics for each class
class_metrics <- sapply(levels(predictions), function(class) calculate_metrics(class, predictions, testData$Diabetes_012))
# Print the metrics for each class
print(class_metrics)
calculate_metrics <- function(class, predictions, actual_labels) {
# Convert to binary format (one-vs-rest approach) and then to factors
binary_predictions <- factor(as.numeric(predictions == class), levels = c(0, 1))
binary_actual_labels <- factor(as.numeric(actual_labels == class), levels = c(0, 1))
# Calculate precision, recall, and F1-score
precision <- posPredValue(binary_predictions, binary_actual_labels, positive = "1")
recall <- sensitivity(binary_predictions, binary_actual_labels, positive = "1")
f1_score <- ifelse((precision + recall) > 0, (2 * precision * recall) / (precision + recall), 0)
return(c(Precision = precision, Recall = recall, F1_Score = f1_score))
}
# Calculating metrics for each class
class_metrics <- sapply(levels(predictions), function(class) calculate_metrics(class, predictions, testData$Diabetes_012))
# Print the metrics for each class
print(class_metrics)
length(predictions)
length(testData$Diabetes_012)
# the tuned model
# extract the best mtry value from the tuned model
best_mtry <- tuned_model$bestTune$mtry
# Model Evaluation
# Generate class predictions
predictions <- predict(model_rf, testData)
# Generate predicted probabilities
prob_predictions <- predict(model_rf, testData, type = "prob")
# Ensure predictions are factors with the same levels as the actual data
predictions <- factor(predictions, levels = levels(testData$Diabetes_012))
# Confusion Matrix
confMat <- confusionMatrix(predictions, testData$Diabetes_012)
print(confMat)
# Overall Accuracy
accuracy <- sum(predictions == testData$Diabetes_012) / length(predictions)
print(paste("Accuracy:", accuracy))
# Function to calculate ROC and AUC for each class
calculate_roc_auc <- function(class, predictions, actual_labels) {
binary_response <- as.numeric(actual_labels == class)
roc_curve <- roc(binary_response, predictions[, as.character(class)])
auc_value <- auc(roc_curve)
list("roc_curve" = roc_curve, "auc" = auc_value)
}
# Calculating ROC and AUC for each class
roc_results <- lapply(levels(testData$Diabetes_012), calculate_roc_auc, prob_predictions, testData$Diabetes_012)
# Printing AUC values for each class
sapply(roc_results, function(x) x$auc)
# Precision, Recall, and F1-Score
# Function to calculate precision, recall, and F1-score for a given class
calculate_metrics <- function(class, predictions, actual_labels) {
# Convert to binary format (one-vs-rest approach) and then to factors
binary_predictions <- factor(as.numeric(predictions == class), levels = c(0, 1))
binary_actual_labels <- factor(as.numeric(actual_labels == class), levels = c(0, 1))
# Calculate precision, recall, and F1-score
precision <- posPredValue(binary_predictions, binary_actual_labels, positive = "1")
recall <- sensitivity(binary_predictions, binary_actual_labels, positive = "1")
f1_score <- ifelse((precision + recall) > 0, (2 * precision * recall) / (precision + recall), 0)
return(c(Precision = precision, Recall = recall, F1_Score = f1_score))
}
# Calculating metrics for each class
class_metrics <- sapply(levels(predictions), function(class) calculate_metrics(class, predictions, testData$Diabetes_012))
# Print the metrics for each class
print(class_metrics)
calculate_metrics <- function(class, predictions, actual_labels) {
# Convert to binary format (one-vs-rest approach) and then to factors
binary_predictions <- factor(as.numeric(predictions == class), levels = c(0, 1))
binary_actual_labels <- factor(as.numeric(actual_labels == class), levels = c(0, 1))
# Calculate precision, recall, and F1-score
precision <- posPredValue(binary_predictions, binary_actual_labels, positive = "1")
recall <- sensitivity(binary_predictions, binary_actual_labels, positive = "1")
f1_score <- ifelse((precision + recall) > 0, (2 * precision * recall) / (precision + recall), 0)
return(c(Precision = precision, Recall = recall, F1_Score = f1_score))
}
# Calculating metrics for each class
class_metrics <- sapply(levels(predictions), function(class) calculate_metrics(class, predictions, testData$Diabetes_012))
# Print the metrics for each class
print(class_metrics)
